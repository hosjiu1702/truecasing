{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48be734d-56b1-454e-af76-2e4a454ee587",
   "metadata": {},
   "source": [
    "__This notebook re-implements paper:__ _https://arxiv.org/pdf/1912.07095.pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59892d0-6c49-476f-a1bf-b24b77a79bed",
   "metadata": {},
   "source": [
    "_Some images about some blocks we will implement._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c20dbc-3a21-401f-888e-a9c7b2f2a070",
   "metadata": {},
   "source": [
    "![img1](images/truecasing-1.png)\n",
    "![img2](images/truecasing-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcbced-d5a3-4f04-90a4-4e2e02acb71e",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f00ef2f6-8654-4194-b16a-b97d7c80f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    X, y = [], []\n",
    "    for sample in data:\n",
    "        tmp = sample.split(\"\\t\")\n",
    "        X.append(tmp[0])\n",
    "        y.append(tmp[1])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3f32-eefb-48c7-8695-a176173190b5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f010025f-71a6-40ae-b109-8bc247369379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf80072d-73fb-47ba-8734-a1c10f5fcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/ner-fullname.txt\"\n",
    "X, y = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6903b91d-2908-41da-a67f-a2e01d1ec7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('họ và tên tôi là trần hoài an huyền', 'trần hoài an huyền|sys.full_name\\n')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a50b3166-07da-4660-8da2-920e019d091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lều chí sình là sếp của tôi', 'lều chí sình|sys.full_name\\n'),\n",
      " ('thái ngô khánh tuyền là mình', 'thái ngô khánh tuyền|sys.full_name\\n'),\n",
      " ('lý a lầy là anh họ tôi', 'lý a lầy|sys.full_name\\n'),\n",
      " ('nguyễn duy là tên của một đồng nghiệp', 'nguyễn duy|sys.full_name\\n'),\n",
      " ('tên trên giấy tờ của tôi là trần đức giang',\n",
      "  'trần đức giang|sys.full_name\\n')]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "pprint([sample for sample in zip(X_train[:5], y_train[:5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ea1ed-3177-403f-9b1c-e2c6918e2db2",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5238ed-97ec-45da-ae36-485fd47173b6",
   "metadata": {},
   "source": [
    "### Trucasing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4a4fa4a-dcaf-4327-a257-53ae569959c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40eac086-f434-454d-9f04-9955e49dc446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b702339e-cb74-4037-8ba2-7f2f150e9285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check GPU\n",
      "\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 10:26:53.789752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 10:26:53.794788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 10:26:53.794960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Check GPU\", end=\"\\n\\n\")\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669fcdd-5dd8-4459-b252-5ad8e051cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This number should be inspected from your own text corpus.\n",
    "# Here it's just a naive assumption.\n",
    "MAXLEN = 90\n",
    "\n",
    "# We need vectorize all of texts available in the dataset\n",
    "# and mapping every single characters to its unique index\n",
    "# in the vocabulary.\n",
    "vectorizer = TextVectorization(split=tf.strings.bytes_split,\n",
    "                               output_mode=\"int\",\n",
    "                               output_sequence_length=MAXLEN)\n",
    "vectorizer.adapt(Dataset.from_tensor_slices(X_train[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86950416-0a39-49c1-93af-bf0afc6e7284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 2 2 1 1 1 1 1 3 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "['', '[UNK]', 'o', 'b', 'a', 'z', 'r', 'f']\n"
     ]
    }
   ],
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 50  # Sequence length to pad the outputs to.\n",
    "\n",
    "# def _chars_split(x):\n",
    "#     splits = []\n",
    "#     for i, _ in enumerate(x):\n",
    "#         chars = np.array(list(x[i])).reshape(1, -1).squeeze()\n",
    "#         print(chars)\n",
    "#         splits.append(chars.tolist())\n",
    "#     return splits\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " split=tf.strings.bytes_split,\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n",
    "# vectorize_layer.adapt(np.array([\"foo\", \"bar\", \"baz\"]))\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
    "print(model.predict(input_data))\n",
    "print(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab68bf-d266-4830-b35f-1f200b86f33d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b8021-7d05-4c8d-b4ab-fa30c09da5f2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3afccf-3f76-466b-b0f9-9e30b9a9723d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
